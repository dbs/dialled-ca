#!/usr/bin/env python

# Copyright (C) 2016 Dan Scott <dscott@laurentian.ca>

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""
Grab JSON data from http://cwrc.ca/rsc-src/ to seed our list of libraries and archives

Data comes in the following format:
{"items": [
    {
        "label": "Alberton Public Library",
        "institutionType": "Library",
        "group": "Libraries",
        "libraryType": "Public",
        "subGroup": "Public libraries",
        "community": "Alberton",
        "provCode": "PE",
        "province": "Prince Edward Island",
        "population": null,
        "startDate": "n/a",
        "url": "http://www.library.pe.ca/libraries",
        "latitude": "46.8128",
        "longitude": "-64.0653",
        "latLng": "46.812804,-64.065297",
        "sort": "1"
    },
"""

import json
import logging
import os
import rdflib
import requests
import socket
import urllib
from rdflib.namespace import DC, DCTERMS, FOAF, OWL, XMLNS

# Workaround bad SSL implementations per http://askubuntu.com/a/488277
import ssl
ssl.PROTOCOL_SSLv23 = ssl.PROTOCOL_TLSv1

def gather_cwrc_data(jsonfile):
    "Gather CRWC data on >4700 LAMs for local processing"
    provinces = ('ab', 'bc', 'mb', 'nb', 'nl', 'ns', 'nt', 'nu', 'on', 'pe', 'qc', 'sk', 'yt')

    # The raw set of province data
    pdata = []
    for x in provinces:
        url = "http://cwrc.ca/rsc-src/datasets/%s.json" % (x)
        pjson = requests.get(url)

        if pjson.status_code < 400:
            # Get the data in UTF8 format
            libs = pjson.json()
            pdata += libs['items']
        else:
            logging.error("Couldn't get %s: %s" % (url, pjson.status_code))

    outf = open(jsonfile, 'w')
    json.dump(pdata, outf, indent=4, sort_keys=True)

def check_lib_urls(data, maxlinks=None, overwrite=False):
    "Test URLs to see if they still resolve"

    tested = []
    headers = {'User-Agent': "Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:43.0) Gecko/20100101 Firefox/43.0"}
    res = {"good": 1, "bad": 1}
    cnt = 0
    session = requests.Session()
    for lib in data:
        cnt += 1

        if maxlinks and cnt > maxlinks:
            continue

        if not lib['url']:
            url_problem("No URL", lib, res)
            continue

        u = clean_url(lib, cnt)

        if u in tested:
            logging.warning("%d\t%s\t%s\tSkipping duplicate" % (cnt, lib['label'], u))
            continue

        tested.append(u)

        if u[0:4] != 'http':
            url_problem("Not an HTTP URL", lib, res)
            continue

        logging.info("%d\t%s\t%s\tChecking..." % (cnt, lib['label'], u))
        try:
            if overwrite is False and os.stat(os.path.join('html', _url_to_filename(u))):
                logging.debug("%d\t%s\t%s\tExists, skipping!" % (cnt, lib['label'], u))
                res['good'] += 1
                continue
        except:
            logging.info("%d\t%s\t%s\tGetting..." % (cnt, lib['label'], u))

        try:
            response = session.get(u, headers=headers, timeout=10)
            code = response.status_code
            if code >= 400:
                url_problem(code, lib, res)
            elif u != response.url:
                logging.info("Redirect: %s\t%s" % (u, response.url))

            save_html(u, response.text)
            res['good'] += 1
        except socket.timeout:
            url_problem("timeout", lib, res)
        except (Exception) as error:
            url_problem(error.__class__.__name__, lib, res)

    return res

def clean_url(lib, cnt=0):
    "Remove spaces and unicode from the URL"

    u = lib['url'].strip().strip('\u200e').strip('\u200f')
    if u != lib['url']:
        logging.warning("%d\t%s\t[%s]\tURL has spaces or RTL/LTR Unicode" % (cnt, lib['label'], lib['url']))

    return u

def save_html(u, text):
    "Save the contents of the URL to disk"
    try:
        os.stat('html')
    except:
        os.mkdir('html')

    fname = _url_to_filename(u)

    try:
        fh = open(os.path.join('html', fname), "w")
        with fh:
            fh.write(text)
    except (Exception) as error:
        logging.error("Couldn't save HTML file named %s: %s" % (fname, error))

def _url_to_filename(u):
    "Convert a URL to a writable filename"
    trans = str.maketrans({"+": "_", " ": "_", ":": "_", "/": "_", "\\": "_", ".": "_"})
    return u.split('?')[0].translate(trans)

def url_problem(problem, lib, r):
    "Abstract the many potential URL problems"
    logging.error("%s\t%s\t%s" % (lib['label'], lib['url'], problem))
    r['bad'] += 1

def correct_base(g, fbase, u):
    "Swap the file system base for the URL base"

    g2 = rdflib.Graph()
    for s, p, o in g:
        path = 'file://'
        if s.startswith(path):
            abspath = urllib.parse.urljoin(path, os.getcwd())
            if s.startswith(abspath):
                path = abspath
            s = rdflib.URIRef(urllib.parse.urljoin(u, s[len(path):]))
        g2.add((s, p, o))
    return g2

def parse_lib_urls(data, maxlinks=None):
    "Extract RDF from RDFa in returned HTML"
    
    cnt = 0
    for lib in data:
        cnt += 1
        if maxlinks and maxlinks > cnt:
            continue

        if not lib['url']:
            continue

        parse_lib_url(lib, cnt)

def bind_namespaces(graph):
    "Bind common namespaces"

    ogp = rdflib.Namespace('http://ogp.me/ns#')
    purlrss = rdflib.Namespace('http://purl.org/rss/1.0/modules/content/')
    rdfa = rdflib.Namespace('http://www.w3.org/ns/rdfa#')
    schema = rdflib.Namespace('http://schema.org/')
    sioc = rdflib.Namespace('http://rdfs.org/sioc/ns#')
    xhtml = rdflib.Namespace('http://www.w3.org/1999/xhtml/vocab#')
    mdata = rdflib.Namespace('http://www.w3.org/ns/md#')

    graph.bind('dc', DC)
    graph.bind('dcterms', DCTERMS)
    graph.bind('md', mdata)
    graph.bind('ogp', ogp)
    graph.bind('rdfa', rdfa)
    graph.bind('schema', schema)
    graph.bind('sioc', sioc)
    graph.bind('xhtml', xhtml)
    graph.bind('foaf', FOAF)
    graph.bind('owl', OWL)
    graph.bind('xmlns', XMLNS)
    graph.bind('purlrss', purlrss)

def parse_lib_url(lib, cnt):
    "Parse and write the graph for a single URL"
    purlrss = rdflib.Namespace('http://purl.org/rss/1.0/modules/content/')
    try:
        rawurl = clean_url(lib)
        fname = os.path.join('html', _url_to_filename(rawurl))
        if not os.stat(fname):
            return
        logging.info("%d\t%s\t%s\tParsing..." % (cnt, lib['label'], lib['url']))
        g = rdflib.Graph()
        # format='html' invokes both RDFa and Microdata parsers
        r = g.parse(open(fname, 'r'), format='html', location=rawurl)
        bind_namespaces(g)

        # Add sameAs assertions for redirected URLs
        if 'urlPrevious' in lib:
            g.add((lib['url'], OWL['sameAs'], lib['urlPrevious']))
            g.add((lib['url'], schema['sameAs'], lib['urlPrevious']))

        out = open("%s.n3" % (fname), "w")
        with out:
            out.write(str(g.serialize(format='n3').decode('utf-8')))

    except (urllib.error.HTTPError) as inst:
        logging.error("%s %s %s" % (inst.code, inst.msg, inst.filename))
    except (Exception) as inst:
        logging.error("%s" % (inst))

def main(maxlinks=None):
    "Main function"
    jsonfile = 'all_libraries.json'
    logfile = 'crawl.log'
    logformat = '%(asctime)s %(levelname)s:%(message)s'

    logging.basicConfig(format=logformat, filename=logfile, level=logging.INFO, datefmt='%Y:%m:%d %H:%M:%S')
    # requests module is very chatty
    logging.getLogger("requests").setLevel(logging.WARNING)
    logging.getLogger("rdflib").setLevel(logging.WARNING)
    logging.info('Starting')

    try:
        jsonp = open(jsonfile, 'r')
    except FileNotFoundError:
        print("First run: getting JSON data.")
        gather_cwrc_data(jsonfile)
        jsonp = open(jsonfile, 'r')

    with jsonp:
        libdata = json.loads(jsonp.read())
        r = check_lib_urls(libdata, maxlinks)
        logging.info("Good: %s\nBad: %s" % (r['good'], r['bad']))
        parse_lib_urls(libdata, maxlinks)
    logging.info('Finished')

if __name__ == '__main__':
    lib = [{
            'url': 'http://library.laurentian.ca/content/library',
            'label': 'Laurentian University'
        },{
            'url': 'http://www.yukonminers.ca/',
            'label': 'Yukon Miners'
        }
    ]
    #parse_lib_urls(lib)
    main(maxlinks=None)

